# -*- coding: utf-8 -*-
"""Interenship Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sETev9W0SDcC9Du9Qeor-O_uGQCtou_-

**Name** - Nihalahmed Munir Barudwale

**Project name** - Prediction of Breast cancer for a given patient
"""

#

from google.colab import drive
drive.mount('/content/drive')

"""**Import Required Libraries**"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
import os
import warnings
warnings.filterwarnings('ignore')
from scipy.stats import norm

"""**Import dataset**"""

dataset = pd.read_csv("/content/drive/MyDrive/TCR Internship Project/data.csv")

dataset

"""**Shape of dataset**"""

dataset.shape

"""**Some Operations on dataset**"""

dataset.head()

dataset.tail()

type(dataset)

dataset.info()

dataset.describe()

dataset.columns

dataset.isna().sum()

dataset.isnull().sum()

"""**Data Preprocessing**

Deleting 'Unnamed:32' column which is of no use to us and it contains only null values.
"""

del dataset['Unnamed: 32']

dataset.shape

dataset.isna().sum()

dataset.diagnosis.unique()

"""From the results above, diagnosis is a categorical variable, because it represents a fix number of possible values (i.e, Malignant, of Benign. The machine learning algorithms wants numbers, and not strings, as their inputs so we need to convert them into numbers."""

dataset['diagnosis'] = dataset['diagnosis'].apply(lambda x : '1' if x == 'M' else '0')
dataset = dataset.set_index('id')

dataset.skew()

"""The skew result show a positive (right) or negative (left) skew. Values closer to zero show less skew. From the graphs, we can see that radius_mean, perimeter_mean, area_mean, concavity_mean and concave_points_mean are useful in predicting cancer type due to the distinct grouping between malignant and benign cancer types in these features. We can also see that area_worst and perimeter_worst are also quite useful."""

#

"""**Exploratory Data Analysis (EDA)**

**Analysing the 'diagnosis' variable**
"""

dataset.diagnosis.describe()

dataset.diagnosis.unique()

dataset.diagnosis.value_counts()

"""1= Malignant (Cancerous) - Present

0= Benign (Not Cancerous) -Absent
"""

print("Percentage of patients which have breast cancer: "+str(round(212*100/569,2)))
print("Percentage of patient which does not have breast cancer: "+str(round(357*100/569,2)))

y = dataset["diagnosis"]
sns.countplot(y)

"""**Get an overview distribution of each column**"""

dataset.hist(figsize=(16, 20), xlabelsize=8, ylabelsize=8)

"""**Separate columns into smaller dataframes to perform visualization**"""

data_id_diag=dataset.reindex(columns=['id', 'diagnosis'])
data_diag=dataset.reindex(columns=['diagnosis'])

#For a merge + slice:
data_mean=dataset.iloc[:,1:11]
data_se=dataset.iloc[:,11:22]
data_worst=dataset.iloc[:,23:]

"""**Visualise distribution of data via histograms**

**1.Histogram the "_mean" suffix designition**
"""

hist_mean=data_mean.hist(bins=10, figsize=(15, 10),grid=False,)

#

"""***2.Histogram for the "_se" suffix designition***"""

hist_se=data_se.hist(bins=10, figsize=(15, 10),grid=False,)

#

"""**3.Histogram "_worst" suffix designition**"""

hist_worst=data_worst.hist(bins=10, figsize=(15, 10),grid=False,)

#

"""**Visualise distribution of data via Density plots**

**1.Density plots "_mean" suffix designition**
"""

plt = data_mean.plot(kind= 'density', subplots=True, layout=(4,3), sharex=False, 
                     sharey=False,fontsize=12, figsize=(15,10))

"""**2.Density plots "_se" suffix designition**"""

plt = data_se.plot(kind= 'density', subplots=True, layout=(4,3), sharex=False, 
                    sharey=False,fontsize=12, figsize=(15,10))

"""**3.Density plot "_worst" suffix designition**"""

plt = data_worst.plot(kind= 'kde', subplots=True, layout=(4,3), sharex=False, sharey=False,fontsize=5, 
                   figsize=(15,10))

"""**Correlation heatmap**"""

dataset.corr()

data_mean.corr()

import pandas as pd
import numpy as np
import seaborn as sns
from matplotlib import pyplot as plt
f, ax = plt.subplots(figsize=(15, 10))
plt.title('Breast Cancer Feature Correlation')
sns.heatmap(data_mean.corr(),annot=True,cmap='PiYG',linewidths=.5)

"""**Splitting the data -
Train Test split**

**Label encoding**
"""

#Here, I assign the 30 features to a NumPy array x, and transform the class labels from their original string representation (M and B) into integers.
array = dataset.values
x = array[:,1:31]
y = array[:,0]

from sklearn.model_selection import train_test_split
array = dataset.values
x = array[:,1:31]
y = array[:,0]
X_train,X_test,Y_train,Y_test = train_test_split(x,y,test_size=0.20,random_state=0)

X_train.shape

X_test.shape

Y_train.shape

Y_test.shape

from sklearn.metrics import accuracy_score

"""**Logistic Regression**"""

from sklearn.linear_model import LogisticRegression
model_logistic_reg = LogisticRegression()
model_logistic_reg.fit(X_train,Y_train)
Y_pred_logistic_reg = model_logistic_reg.predict(X_test)

Y_pred_logistic_reg.shape

print("Predicted Values : ",Y_pred_logistic_reg)

Y_test[0:10] #You can check accuracy by observing predicted results and test data.

accuracy_score_logistic_reg = round(accuracy_score(Y_pred_logistic_reg,Y_test)*100,2)
print("The accuracy score achieved using Logistic Regression is: "+str(accuracy_score_logistic_reg)+" %")

"""**SVM**"""

from sklearn import svm
model_svm = svm.SVC(kernel='linear')
model_svm.fit(X_train, Y_train)
Y_pred_svm = model_svm.predict(X_test)

Y_pred_svm.shape

print("Predicted Values : ",Y_pred_svm)

Y_test[0:10] #You can check accuracy by observing predicted results and test data.

accuracy_score_svm = round(accuracy_score(Y_pred_svm,Y_test)*100,2)
print("The accuracy score achieved using Linear SVM is: "+str(accuracy_score_svm)+" %")

"""**K Nearest Neighbors**"""

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=7)
knn.fit(X_train,Y_train)
Y_pred_knn=knn.predict(X_test)

Y_pred_knn.shape

print("Predicted Values : ",Y_pred_knn)

Y_test[0:10] #You can check accuracy by observing predicted results and test data.

accuracy_score_knn = round(accuracy_score(Y_pred_knn,Y_test)*100,2)
print("The accuracy score achieved using KNN is: "+str(accuracy_score_knn)+" %")

"""**Decision Tree**"""

from sklearn.tree import DecisionTreeClassifier
max_accuracy = 0
for x in range(200):
    dt = DecisionTreeClassifier(random_state=x)
    dt.fit(X_train,Y_train)
    Y_pred_dt = dt.predict(X_test)
    current_accuracy = round(accuracy_score(Y_pred_dt,Y_test)*100,2)
    if(current_accuracy>max_accuracy):
        max_accuracy = current_accuracy
        best_x = x
        
dt = DecisionTreeClassifier(random_state=best_x)
dt.fit(X_train,Y_train)
Y_pred_dt = dt.predict(X_test)

print(Y_pred_dt.shape)

print("Predicted Values : ",Y_pred_dt)

Y_test[0:10] #You can check accuracy by observing predicted results and test data.

accuracy_score_dt = round(accuracy_score(Y_pred_dt,Y_test)*100,2)
print("The accuracy score achieved using Decision Tree is: "+str(accuracy_score_dt)+" %")

"""**Random Forest**"""

from sklearn.ensemble import RandomForestClassifier
max_accuracy = 0
for x in range(2000):
    rf = RandomForestClassifier(random_state=x)
    rf.fit(X_train,Y_train)
    Y_pred_rf = rf.predict(X_test)
    current_accuracy = round(accuracy_score(Y_pred_rf,Y_test)*100,2)
    if(current_accuracy>max_accuracy):
        max_accuracy = current_accuracy
        best_x = x
        
rf = RandomForestClassifier(random_state=best_x)
rf.fit(X_train,Y_train)
Y_pred_rf = rf.predict(X_test)

Y_pred_rf.shape

print("Predicted Values : ",Y_pred_rf)

Y_test[0:10] #You can check accuracy by observing predicted results and test data.

accuracy_score_rf = round(accuracy_score(Y_pred_rf,Y_test)*100,2)
print("The accuracy score achieved using Random Forest is: "+str(accuracy_score_rf)+" %")

"""**Summary of accuracy scores**"""

all_accuracy_scores = [accuracy_score_logistic_reg,accuracy_score_svm,accuracy_score_knn,accuracy_score_dt,accuracy_score_rf]
algorithms_used = ["Logistic Regression","Support Vector Machine","K-Nearest Neighbors","Decision Tree","Random Forest"]

for i in range(len(algorithms_used)):
    print("\nThe accuracy score achieved using "+algorithms_used[i]+" is: "+str(all_accuracy_scores[i])+" %")

sns.set(rc={'figure.figsize':(15,8)})
plt.xlabel("Algorithms")
plt.ylabel("Accuracy score")

sns.barplot(algorithms_used,all_accuracy_scores)

"""**Here we can see that Random Forest is better than other algorithms.**


---

**Name** - Nihalahmed Munir Barudwale

**Project name** - Prediction of Breast cancer for a given patient
"""